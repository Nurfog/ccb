# Gu√≠a de Uso: GPU vs CPU

El ML Service de CCB puede funcionar tanto con GPU (acelerado) como con CPU (sin GPU), adapt√°ndose autom√°ticamente al hardware disponible.

## Modos de Ejecuci√≥n

### üöÄ Modo 1: CPU-only (Por Defecto)

**Ideal para**: 
- Desarrollo sin GPU
- Servidores sin GPU
- Testing y debugging
- Modelos peque√±os

**C√≥mo usar**:
```bash
# Simplemente ejecuta docker-compose normalmente
docker compose up -d

# El ML service usar√° CPU autom√°ticamente
```

**Caracter√≠sticas**:
- ‚úÖ No requiere NVIDIA GPU
- ‚úÖ Funciona en cualquier m√°quina
- ‚úÖ Imagen Docker m√°s ligera (~2GB vs ~8GB)
- ‚ö†Ô∏è Entrenamiento m√°s lento (10-100x dependiendo del modelo)

---

### ‚ö° Modo 2: GPU Acelerado (RTX 2070 Super)

**Ideal para**:
- Entrenamiento de modelos grandes
- Procesamiento de datasets masivos
- Producci√≥n con alta demanda
- Deep Learning con redes neuronales complejas

**Requisitos**:
1. NVIDIA GPU (en tu caso: RTX 2070 Super ‚úì)
2. NVIDIA drivers instalados
3. NVIDIA Container Toolkit

**C√≥mo usar**:
```bash
# Usar el perfil GPU
docker compose --profile gpu up -d

# Verifica que est√© usando GPU
curl http://localhost:8000/health
```

**Caracter√≠sticas**:
- ‚úÖ Entrenamiento ultra-r√°pido
- ‚úÖ Puede procesar batches grandes
- ‚úÖ Ideal para producci√≥n
- ‚ö†Ô∏è Requiere setup de GPU

---

### üîÑ Modo 3: Autodetecci√≥n (Recomendado para desarrollo)

**C√≥mo funciona**:
El servicio detecta autom√°ticamente si hay GPU disponible y usa la m√°s apropiada.

**Configurar en .env**:
```env
ML_DEVICE=auto  # Detecta autom√°ticamente (por defecto)
```

**Ventajas**:
- Mismo c√≥digo funciona en cualquier entorno
- Cambia de GPU a CPU autom√°ticamente si falla
- Ideal para desarrollo h√≠brido

---

## Comparaci√≥n de Rendimiento

### Ejemplo: Entrenar modelo de regresi√≥n con 100k filas

| Hardware | Tiempo | Costo | Uso Recomendado |
|----------|--------|-------|-----------------|
| **RTX 2070 Super** | ~30s | GPU | Producci√≥n, modelos grandes |
| **CPU (8 cores)** | ~5min | CPU | Desarrollo, modelos peque√±os |
| **CPU (4 cores)** | ~10min | CPU | Testing b√°sico |

---

## Comandos √ötiles

### Iniciar con CPU (sin GPU)
```bash
# M√©todo 1: Por defecto
docker compose up -d

# M√©todo 2: Expl√≠cito con perfil
docker compose --profile cpu up -d
```

### Iniciar con GPU
```bash
docker compose --profile gpu up -d
```

### Cambiar de CPU a GPU (sin reconstruir todo)
```bash
# Detener servicio actual
docker compose down ml_service

# Iniciar con GPU
docker compose --profile gpu up -d ml_service_gpu
```

### Cambiar de GPU a CPU
```bash
docker compose down ml_service_gpu
docker compose up -d ml_service
```

### Verificar qu√© modo est√° activo
```bash
curl http://localhost:8000/health | jq '.'
```

Ver√°s algo como:
```json
{
  "status": "healthy",
  "cuda_available": true,  // ‚Üê false si es CPU
  "gpu": {
    "name": "NVIDIA GeForce RTX 2070 SUPER",
    "memory_allocated": "0.00 GB",
    "memory_reserved": "0.00 GB"
  },
  "pytorch_version": "2.2.0+cu121"  // ‚Üê cpu si es CPU-only
}
```

---

## Configuraci√≥n Manual (Avanzado)

Si quieres forzar un dispositivo espec√≠fico independientemente de la detecci√≥n:

**Edita `ml_service/.env`**:
```env
# Forzar GPU (falla si no est√° disponible)
ML_DEVICE=cuda

# O forzar CPU (ignora GPU incluso si est√° disponible)
ML_DEVICE=cpu

# O autodetecci√≥n inteligente
ML_DEVICE=auto
```

---

## Troubleshooting

### ‚ùì ¬øC√≥mo s√© qu√© modo estoy usando?

```bash
docker logs ccb_ml_service | grep "Detectada\|CPU"
```

Ver√°s:
- `‚úì GPU Detectada: NVIDIA...` ‚Üí Modo GPU
- `‚Ñπ Usando CPU para procesamiento ML` ‚Üí Modo CPU

### ‚ùì Tengo GPU pero quiero usar CPU para testing

```bash
# Opci√≥n 1: Variable de entorno
export ML_DEVICE=cpu
docker compose up -d ml_service

# Opci√≥n 2: Directamente en docker-compose
# (edita docker-compose.yml y cambia ML_DEVICE=cpu)
```

### ‚ùì El servicio no arranca con GPU

1. Verifica drivers NVIDIA:
   ```bash
   nvidia-smi
   ```

2. Verifica Container Toolkit:
   ```bash
   docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
   ```

3. Si falla, usa modo CPU temporalmente:
   ```bash
   docker compose up -d ml_service
   ```

---

## Recomendaci√≥n de Uso

```mermaid
graph TD
    A[¬øTienes NVIDIA GPU?] -->|S√≠| B[¬øNecesitas entrenar modelos grandes?]
    A -->|No| C[Usa CPU-only]
    B -->|S√≠| D[Usa GPU Profile]
    B -->|No| E[¬øDesarrollo o Producci√≥n?]
    E -->|Desarrollo| F[Auto-detecci√≥n]
    E -->|Producci√≥n| D
```

**Para tu caso (RTX 2070 Super)**:
- **Desarrollo**: `ML_DEVICE=auto` (detecta autom√°ticamente)
- **Producci√≥n**: `docker compose --profile gpu up -d`
- **Testing**: `ML_DEVICE=cpu` (m√°s predecible)

---

## Migraci√≥n entre Modos

### De CPU a GPU (cuando instalas GPU)
```bash
# 1. Instala NVIDIA Container Toolkit (ver GPU_SETUP.md)
# 2. Reconstruye con GPU
docker compose down
docker compose --profile gpu up -d
# 3. Verifica
curl http://localhost:8000/health
```

### De GPU a CPU (cuando migras a servidor sin GPU)
```bash
# 1. Cambia a perfil CPU
docker compose down
docker compose up -d  # usa CPU por defecto
# 2. El c√≥digo sigue funcionando igual
```

¬°El mismo c√≥digo funciona en ambos casos! üéâ
